[Database]
# Database connection settings
DATABASE_URL = postgresql://postgres:1e3Xdfsdf23@90.156.204.42:5432/postgres

[Scrapy]
# Scrapy project path
SCRAPY_PROJECT_PATH = news_parser
# Python executable path
PYTHON_PATH = python

[Scheduler]
# Spider execution start time (24-hour format)
# Group 1 starts at this time, Group 2 starts 2 hours later
START_HOUR = 10
START_MINUTE = 0

# Timezone for scheduling (optional, defaults to system timezone)
# TIMEZONE = UTC

# Logging level
LOG_LEVEL = INFO

# Batch scheduling settings
# Number of spiders to run concurrently in each batch
BATCH_SIZE = 10
# Interval between cycles in hours (minimum 2 hours)
# Example: 4 = every 4 hours, 6 = every 6 hours, 12 = twice daily
CYCLE_INTERVAL_HOURS = 4
# Number of times to repeat the cycle (0 = run indefinitely)
# Example: 0 = forever, 3 = run 3 times then stop
CYCLE_COUNT = 0

[Web]
# Flask web server settings
HOST = 0.0.0.0
PORT = 5001
DEBUG = True

[Spider]
# Spider execution settings
# Maximum concurrent spiders (11 = limit to 11 at a time across ALL groups)
# This ensures system stability and prevents resource overload
MAX_CONCURRENT = 11
# Spider timeout in seconds
TIMEOUT = 3600
# Retry failed spiders
RETRY_FAILED = True
# Maximum retry attempts
MAX_RETRIES = 3 